{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yaml(yaml_file):\n",
    "    '''read yaml file \n",
    "    inpput - config file name and path'''\n",
    "    data_dict = None\n",
    "    with open(yaml_file) as f:\n",
    "        data_dict = yaml.safe_load(f)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data config file\n",
    "yaml_file_name = 'data_config.yaml'\n",
    "data_dict = read_yaml(yaml_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the config params for data\n",
    "test_percent = int(data_dict['test_percent'])\n",
    "author_data_choice = data_dict['author_data_choice']\n",
    "data_augmentation = data_dict['data_augmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ====== data config ======= \n",
      "test percent :  30 <class 'int'>\n",
      "author_data_choice :  True <class 'bool'>\n",
      "data_augmentation :  True <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "# print the config received for data\n",
    "print(\" ====== data config ======= \")\n",
    "print(\"test percent : \", test_percent , type(test_percent))\n",
    "print(\"author_data_choice : \", author_data_choice, type(author_data_choice))\n",
    "print(\"data_augmentation : \", data_augmentation, type(data_augmentation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the SVM config file\n",
    "yaml_file_name = 'SVM_classifier_config.yaml'\n",
    "data_dict = read_yaml(yaml_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the config params for SVM\n",
    "kernel_name_SVM = data_dict['kernel_name_SVM']\n",
    "balanced_SVM = data_dict['balanced_SVM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SVM Classifier config ==========\n",
      "kernel_name_SVM :  sigmoid <class 'str'>\n",
      "balanced_SVM :  False <class 'bool'>\n"
     ]
    }
   ],
   "source": [
    "# print the config received for SVM\n",
    "print(\"======== SVM Classifier config ==========\")\n",
    "print(\"kernel_name_SVM : \", kernel_name_SVM, type(kernel_name_SVM))\n",
    "print(\"balanced_SVM : \", balanced_SVM, type(balanced_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the MLP config file\n",
    "yaml_file_name = 'MLP_classifier_config.yaml'\n",
    "data_dict = read_yaml(yaml_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the config params for MLP, as number of neruons in layer mentioned as (,,) we need to convert them to list to give it to further MLP function\n",
    "hidden_layer_sizes_MLP = []\n",
    "layer_sizes = data_dict['hidden_layer_sizes_MLP'].split(\",\") # split on \",\"\n",
    "for i in range(len(layer_sizes)):\n",
    "    size = layer_sizes[i]\n",
    "    if i ==0:\n",
    "        hidden_layer_sizes_MLP.append(int(size[1:])) # for first remove ( in (no_neuron and append to list\n",
    "    elif i == len(layer_sizes)-1:\n",
    "        hidden_layer_sizes_MLP.append(int(size[:-1])) # for last remove ) in no_neuron) and append to list\n",
    "    else:\n",
    "         hidden_layer_sizes_MLP.append(int(size))  #  append no_neuron to list\n",
    "hidden_layer_sizes_MLP = tuple(hidden_layer_sizes_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the config hyperparams for MLP\n",
    "learning_rate_init_MLP=data_dict['learning_rate_init_MLP']\n",
    "max_iter_MLP=data_dict['max_iter_MLP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= MLP classifier config =========== \n",
      "hidden_layer_sizes_MLP :  (64, 32) <class 'tuple'>\n",
      "learning_rate_init_MLP :  0.01 <class 'float'>\n",
      "max_iter_MLP :  100 <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# print the config received for MLP\n",
    "print(\"========= MLP classifier config =========== \")\n",
    "print(\"hidden_layer_sizes_MLP : \" , hidden_layer_sizes_MLP, type(hidden_layer_sizes_MLP))\n",
    "print(\"learning_rate_init_MLP : \", learning_rate_init_MLP, type(learning_rate_init_MLP))\n",
    "print(\"max_iter_MLP : \", max_iter_MLP, type(max_iter_MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read yaml config file for test \n",
    "yaml_file_name = 'test_config.yaml'\n",
    "data_dict = read_yaml(yaml_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the config params for test\n",
    "row_from_author_data = data_dict['row_from_author_data']\n",
    "test_question = data_dict['test_question']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== test config ========\n",
      "row_from_author_data :  [5, 15, 150] <class 'list'>\n",
      "test_question :  how bad is covid pandemic <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# print the params for test \n",
    "print(\"===== test config ========\" )\n",
    "print(\"row_from_author_data : \", row_from_author_data , type(row_from_author_data))\n",
    "print(\"test_question : \", test_question, type(test_question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_classifier(kernel_name, class_balanced = False):\n",
    "    '''invokes appropriate SVM classfier\n",
    "    input - kernel to use, whether to balance class'''\n",
    "    if class_balanced == True:\n",
    "        # SVC with particular kernal and class balance = True\n",
    "        SVC1 = SVC(kernel=kernel_name, class_weight='balanced')\n",
    "        return SVC1\n",
    "    else:\n",
    "        # SVC with particular kernal and class balance = False(default)\n",
    "        SVC1 = SVC(kernel=kernel_name)\n",
    "        return SVC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_score(X,y, classifier='SVC'):\n",
    "    '''get accuracy for mentioned classifier \n",
    "    input - input, correct output, classifier (default - SVC) - SVC or MLP'''\n",
    "    if (classifier=='SVC'):\n",
    "        # predict using SVC and input\n",
    "        predict = svcclassifier.predict(X)\n",
    "        # return accuracy score\n",
    "        return accuracy_score(y, predict)\n",
    "    if (classifier=='MLP'):\n",
    "         # predict using MLP and input\n",
    "        predict = mlpclassifier.predict(X)\n",
    "        # return accuracy score\n",
    "        return accuracy_score(y, predict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_data(sample, classifier='SVC'):\n",
    "    '''return prediction for test question using classifier \n",
    "    input - test sample question, classifier'''\n",
    "    if (classifier=='SVC'):\n",
    "        # fit count vectoriser and tf-idf vectorizer to input sentence\n",
    "        test_ex_count = count_vect.transform([sample])\n",
    "        test_ex_tfidf = tfidf_transformer.transform(test_ex_count)\n",
    "        # return the prediction of class using SVC\n",
    "        return svcclassifier.predict(test_ex_tfidf)[0]\n",
    "    if (classifier=='MLP'):\n",
    "        # fit count vectoriser and tf-idf vectorizer to input sentence\n",
    "        test_ex_count = count_vect.transform([sample])\n",
    "        test_ex_tfidf = tfidf_transformer.transform(test_ex_count)\n",
    "        # return the prediction of class using MLP\n",
    "        return svcclassifier.predict(test_ex_tfidf)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_data(augmented=False):\n",
    "    '''get data for training\n",
    "    input - augmented param as boolean'''\n",
    "    if (augmented==True):\n",
    "        # read augmented trained data if augmented = True \n",
    "        df_train = pd.read_csv(\"data_generated/train_data_augmented.csv\")\n",
    "        print(\"train data shape : \", df_train.shape)\n",
    "        return df_train\n",
    "    else:\n",
    "         # read original non-augmented trained data if augmented = False \n",
    "        df_train = pd.read_csv(\"data_generated/train_data.csv\")\n",
    "        print(\"train data shape : \", df_train.shape)\n",
    "        return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(author=True):\n",
    "    '''get test data, source generated to author generated\n",
    "    input - author generated boolean'''\n",
    "    if (author==True):\n",
    "        # if author = True get both test data and author generated test data\n",
    "        df_test = pd.read_csv(\"./data_generated/test_data.csv\")\n",
    "        df_test_author = pd.read_csv(\"./data_generated/test_data_author_generated.csv\")\n",
    "        return df_test, df_test_author\n",
    "    else:\n",
    "        # if author = False get only test data\n",
    "        df_test = pd.read_csv(\"./data_generated/test_data.csv\")\n",
    "        return df_test, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results_on_data(row_from_author_data, classifier='SVC'):\n",
    "    '''print the predictions for data \n",
    "    input - row numbers from author generated data, classifier to use'''\n",
    "    for i in row_from_author_data:\n",
    "        # if row more than number of rows in author generated data set it max row number\n",
    "        if (i > 248):\n",
    "            i = 248\n",
    "        # if author choice true , get data for that row number and predict using given classifier, default, SVC\n",
    "        if author_data_choice == True:\n",
    "            sample = df_test_author['Question'].iloc[i]\n",
    "            print(\"Question : \", df_test_author['Question'].iloc[i])\n",
    "            print(\"predicted : \", test_on_data(sample , classifier))\n",
    "            print(\"correct : \" , df_test_author['Category'].iloc[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf feature generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape :  (2887, 3)\n"
     ]
    }
   ],
   "source": [
    "# get trained data\n",
    "df_train = get_trained_data(augmented=data_augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(df_train['Question'])\n",
    "# X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf transform on count vectorizer data\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "# X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['Category']\n",
    "# y_train\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shapes for train data :  (2887, 3840) (2887,)\n"
     ]
    }
   ],
   "source": [
    "print(\"feature shapes for train data : \", X_train_tfidf.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data shape :  (691, 2)\n",
      "author test data shape :  (249, 2)\n"
     ]
    }
   ],
   "source": [
    "# get test data, print shapes according to choice of test dsata - author test data or only test data\n",
    "df_test, df_test_author = get_test_data(author_data_choice)\n",
    "print(\"test data shape : \", df_test.shape)\n",
    "if author_data_choice == True:\n",
    "    print(\"author test data shape : \", df_test_author.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved count vectorizer and tf-idf transform on test data\n",
    "test_count = count_vect.transform(df_test['Question'])\n",
    "X_test_tfidf = tfidf_transformer.transform(test_count)\n",
    "y_test = df_test['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shapes for test data :  (691, 3840) (691,)\n"
     ]
    }
   ],
   "source": [
    "print(\"feature shapes for test data : \", X_test_tfidf.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature shapes for author test data :  (249, 3840) (249,)\n"
     ]
    }
   ],
   "source": [
    "# saved count vectorizer and tf-idf transform on author test data if we are using it\n",
    "X_test_author_tfidf = None\n",
    "y_test_author = None\n",
    "if (author_data_choice == True):\n",
    "    df_test_author['Question'] = df_test_author['Question'].astype(str)\n",
    "    test_author_count = count_vect.transform(df_test_author['Question'])\n",
    "    X_test_author_tfidf = tfidf_transformer.transform(test_author_count)\n",
    "    y_test_author = df_test_author['Category']\n",
    "    print(\"feature shapes for author test data : \", X_test_author_tfidf.shape, y_test_author.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize classifier using kernel name and class balance\n",
    "svcclassifier = SVM_classifier(kernel_name_SVM, balanced_SVM )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='sigmoid')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the classifier\n",
    "svcclassifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy :  0.9954970557672325\n"
     ]
    }
   ],
   "source": [
    "# get training accuracy \n",
    "train_accuracy = get_accuracy_score(X_train_tfidf, y_train)\n",
    "print(\"train accuracy : \", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy :  0.48625180897250364\n"
     ]
    }
   ],
   "source": [
    "# get testing accuracy \n",
    "\n",
    "test_accuracy = get_accuracy_score(X_test_tfidf, y_test)\n",
    "print(\"test accuracy : \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author test accuracy :  0.4819277108433735\n"
     ]
    }
   ],
   "source": [
    "# get testing accuracy on author generated text data\n",
    "\n",
    "if author_data_choice == True:\n",
    "    test_author_accuracy = get_accuracy_score(X_test_author_tfidf, y_test_author)\n",
    "    print(\"author test accuracy : \", test_author_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  is covid the end of humanity\n",
      "predicted :  Societal Effects \n",
      "correct :  Societal Effects \n",
      "Question :  how is covid prevented\n",
      "predicted :  Testing \n",
      "correct :  Prevention \n",
      "Question :  how can i tell if i have the flu or covid\n",
      "predicted :  Comparison \n",
      "correct :  Comparison \n"
     ]
    }
   ],
   "source": [
    "# print results on data for mentined rows in test config file\n",
    "print_results_on_data(row_from_author_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  how bad is covid pandemic\n",
      "predicted :  Economic Effects \n"
     ]
    }
   ],
   "source": [
    "# test on given test_question\n",
    "pred = test_on_data(test_question)\n",
    "print(\"Question : \", test_question)\n",
    "print(\"predicted : \", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize MLP classifier using number of layers\n",
    "mlpclassifier = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes_MLP, learning_rate_init=learning_rate_init_MLP, max_iter=max_iter_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(64, 32), learning_rate_init=0.01,\n",
       "              max_iter=100)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the classifier\n",
    "mlpclassifier.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy :  0.9996536196744025\n"
     ]
    }
   ],
   "source": [
    "# get training accuracy \n",
    "train_accuracy = get_accuracy_score(X_train_tfidf, y_train, 'MLP')\n",
    "print(\"train accuracy : \", train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy :  0.5224312590448625\n"
     ]
    }
   ],
   "source": [
    "# get accuracy on test data\n",
    "test_accuracy = get_accuracy_score(X_test_tfidf, y_test,'MLP')\n",
    "print(\"test accuracy : \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author test accuracy :  0.4457831325301205\n"
     ]
    }
   ],
   "source": [
    "# get accuracy on author generated test data\n",
    "if author_data_choice == True:  \n",
    "    test_author_accuracy = get_accuracy_score(X_test_author_tfidf, y_test_author,'MLP')\n",
    "    print(\"author test accuracy : \", test_author_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  is covid the end of humanity\n",
      "predicted :  Societal Effects \n",
      "correct :  Societal Effects \n",
      "Question :  how is covid prevented\n",
      "predicted :  Testing \n",
      "correct :  Prevention \n",
      "Question :  how can i tell if i have the flu or covid\n",
      "predicted :  Comparison \n",
      "correct :  Comparison \n"
     ]
    }
   ],
   "source": [
    "# print results on data for mentined rows in test config file\n",
    "print_results_on_data(row_from_author_data,'MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question :  how bad is covid pandemic\n",
      "predicted :  Economic Effects \n"
     ]
    }
   ],
   "source": [
    "# test on given test_question\n",
    "pred = test_on_data(test_question,'MLP')\n",
    "print(\"Question : \", test_question)\n",
    "print(\"predicted : \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
